# Вариант 2В

## Модель параллельного алгоритма

```
D = (3; -2)
d = (<, >)
По внешнемму циклу - истинная зависимость с расстоянием 3 --> можно распаралелить на 3 независимых потока.

По внутреннему - антизависимость, ее можно распараллелить на произвольное число потоков, но при копировании переменных и синхронизации. Т.к. используется модель с распределенной памятью (MPI), то локализация переменных будет происходить в любом случае, и эта проблема исчезает.
```

Для параллельного исполнения применялись функции:

* 2 потока - функция `dual` - две строки вычисляет, третью передает второму процессу, а потом загружает обратно.
* 3 - потока - функция `triple` - каждый процесс с шагом в 3 вычисляет строки массива, 1й процесс при этом также отвечает за выгрузку строк другим процессам и их получение.
* 4 и более потоков - функция `multi` - процесс 0 отвечает за выгрузку строк матрицы в процессы 1, 2, 3 (мастер). Последние в свою очередь, если больше нет процессов, просто их вычисляют и отправляют процессу 0. Если же процессов больше, то процессы 1,2,3 используют их (4, 5, ...) для распараллеливания вычислений по внутреннему циклу по принципу окна:

> Размер окна определяет число строк, которые будут делиться на блоки для параллельных вычислений по внутреннему циклу. При этом число перегород в строках больше или равно числу процессов для параллельных вычислений по внутреннему циклу (4, 5, ...)

Пример:

Пусть число потоков 12, размер окна 6. Тогда 12-4=8 потоков уйдет на параллелизацию внутреннего цикла в пределах 6-ти строк:

![image](https://user-images.githubusercontent.com/55103017/145726725-c817c536-7007-4e2e-babc-868b80989aa4.png)


Здесь зеленый - номер процесса, который будет обрабатывать один блок ДО перегородки. Самый правый обрабатывается всегда внешними процессами (1,2,3), а 4, 5, 6, ... после обработки своих кусочков пересылают его этм внешним процессам. Когда строка полностью готова, она пересылается матстеру (процессу 0).

Окно по мере загрузки строк процессами 1,2,3 сдвигается вниз.

Размер окна определяет частоту делений строк, вот так бы выглядела картина при окне 3:

![image](https://user-images.githubusercontent.com/55103017/145726745-0cbeaa40-4db6-4ec1-bbf3-7cef8f9903b2.png)

## Измерение времени

Измерение времени проводилось с учетом всех пересылок данных при вычислениях матрицы. Выделение памяти под матрицу процессом 0 и ее инициализация не входила в расчет (для одного и более процессов).

Хорошее ускорение было получено только на 2,3,4 числе потоков. Далее из-за медленной работы буферов (скорость обмена данными м/д нодами) ускорение с увеличением числа потоков получилось меньше единицы. Эту гипотезу подтверждает тот факт, что параллельная программа работала в среднем эффективнее при размере окна, соизмеримым с общим числом строк - в этом случае первые N строк (N - число процессов - 4; N <= 32) делились бы пополам и обрабатывались разными процессами, общее число пересылок при этом минимально.

```С
// ускорение
S = T(1) / T(p)
// еффективность
E = S(p) / p
```

![image](https://user-images.githubusercontent.com/55103017/145726765-5bf77ffb-276e-4c85-a338-df8b67eb4706.png)
![image](https://user-images.githubusercontent.com/55103017/145726781-50f0947e-7826-48ee-9aa3-3a5fa418ed2e.png)


## Запуск программы

После загрузки всей папки на кластер запустить:

```
./build.sh
```

